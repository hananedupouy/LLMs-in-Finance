{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEDuDC-wD5xj"
   },
   "source": [
    "# How well can Multimodal LLMs interpret complex financial data? Letâ€™s find out!\n",
    "\n",
    "â–¶ I recently conducted an experiment with 5 different Multimodal LLMs, and sizes, to see how they handle interpreting a complex financial chart. The models tested were: **Llama-3.2-11B-Vision, Pixtral-12B, Qwen 2 VL 2B**, and the heavyweights: **Claude 3.5 Sonnet and GPT-4o**.\n",
    "\n",
    "â–¶ The chart I used was taken from JP Morgan's 2022 report, featuring multiple data types and visual elements like bar and line graphsâ€”a real test of the models' ability to process intricate financial information.\n",
    "\n",
    "â–¶ Why does this matter? In finance, being able to accurately interpret visuals and numerical data is critical. I wanted to assess:\n",
    "\n",
    "*   How well these models handle mixed data formats.\n",
    "*   Whether they can interpret complex financial values.\n",
    "*   How feasible it is to use them in real-world financial analysis, despite varying model sizes and architectures.\n",
    "\n",
    "â–¶ Even though these models differ in size and complexity, the ultimate goal was to determine their potential when working with numbers and visual data. Some fascinating insights emerged!\n",
    "\n",
    "â–¶ How I run them?\n",
    "\n",
    "*   I have run a quantized version of Llama-3.2-11B-Vision, on Google colab, using GPU (Runtime ==> Change Runtime Type ==> GPU)\n",
    "\n",
    "*   I used MistralAI for Pixtral-12B (Mistral AI Key + subscribe to free usage)\n",
    "\n",
    "*   I used HuggingFace for Qwen2VL\n",
    "\n",
    "*   I used OpenAI and Anthropic API for GPT-4o and Claude 3.5 Sonnet  \n",
    "\n",
    "\n",
    "â–¶ Key Takeaways:\n",
    "\n",
    "*   Well, the largest models excel in extracting the correct numbers from this complex chart. However, they sometimes do not extract the whole expected values (For example, they ommit to extract the first table, or they don't capture the whole years from the chart...)\n",
    "\n",
    "*   The smallest models, while they can capture all the metrics included in the chart, they don't extract yet the accurate numbers...I believe they are good at describing images, but not yet for exact numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWAA4ch-DCiD"
   },
   "source": [
    "Here is the image from which we want multimodal LLMs to extract numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "h212Nnfz9GLi",
    "outputId": "f65ed371-efd5-4f4d-8d12-313a6d318201"
   },
   "outputs": [],
   "source": [
    "# path_img = local_path\n",
    "from IPython.display import Image\n",
    "Image(path_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miHsR4imzlud"
   },
   "source": [
    "# Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkoXj8wU8rVe",
    "outputId": "c58e05f0-01fa-418c-d7ea-a7f635f208cf"
   },
   "outputs": [],
   "source": [
    "# !pip install anthropic -q\n",
    "# !pip install openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rEcA_sUDrdE"
   },
   "source": [
    "# Specify Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSlvkQxD8_DA"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "CLAUDE_API_KEY = userdata.get('CLAUDE_API_KEY')\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "MISTRAL_API_KEY = userdata.get('MISTRAL_API_KEY')\n",
    "\n",
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4RQaQcpDovI"
   },
   "source": [
    "# Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKsPJFXd-XUf"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "# Convert the PNG images to base64 encoded strings: One example images\n",
    "images = [Image.open(f\"{path_img}\")]\n",
    "\n",
    "base64_encoded_pngs = []\n",
    "quality=75\n",
    "max_size=(1024, 1024)\n",
    "for image in images:\n",
    "        # Resize the image if it exceeds the maximum size\n",
    "        if image.size[0] > max_size[0] or image.size[1] > max_size[1]:\n",
    "            image.thumbnail(max_size, Image.Resampling.LANCZOS)\n",
    "        image_data = io.BytesIO()\n",
    "        image.save(image_data, format='PNG', optimize=True, quality=quality)\n",
    "        image_data.seek(0)\n",
    "        base64_encoded = base64.b64encode(image_data.getvalue()).decode('utf-8')\n",
    "        base64_encoded_pngs.append(base64_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WC9wQoEaDW_1"
   },
   "source": [
    "# GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bLvPL1B-cE4"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "MODEL_NAME_GPT = \"gpt-4o-mini\"\n",
    "\n",
    "def get_completion_gpt4o(messages, model_name):\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        # max_tokens=2048,\n",
    "        temperature=0,\n",
    "        messages=messages\n",
    "    )\n",
    "    print(response.model)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def append_message(content, question):\n",
    "    content.append({\"type\": \"text\", \"text\": question})\n",
    "    messages = [\n",
    "      {\n",
    "          \"role\": 'user',\n",
    "          \"content\": content\n",
    "      }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zanG-_rw2mqO"
   },
   "source": [
    "## Prompt 1: raw data without explicit format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAvz9oJo2tlp"
   },
   "source": [
    "ðŸ”½ â¬‡ : It didn't to extract the first table: ðŸ”½ â¬‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS66tIMp63FU"
   },
   "source": [
    "The extracted data is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEM9j5Au-lCq"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{encoded_png}\"}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract raw data from the image.\"\n",
    "messages_gpt = append_message(content, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2p683cQ08PX",
    "outputId": "1f5ec622-26c3-4397-e1d7-7cb1d62253a8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL_NAME_GPT = \"gpt-4o\"\n",
    "print(get_completion_gpt4o(messages_gpt, MODEL_NAME_GPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJG8vJ252hVn"
   },
   "source": [
    "## Prompt: Json format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o-m6lgn3SOd"
   },
   "source": [
    "ðŸ”½ â¬‡ All data are extracted :ðŸ”½ â¬‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY2UTeye3Wjv"
   },
   "source": [
    "âœ… Numbers coming from the chart are **GOOD** âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oYbYNA-1tB-",
    "outputId": "721519d9-053c-4cb0-b603-969936476e2f"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_png}\"}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract ALL raw data from the image in a json format.\"\n",
    "messages_gpt = append_message(content, question)\n",
    "# MODEL_NAME_GPT = \"gpt-4o\"\n",
    "print(get_completion_gpt4o(messages_gpt, MODEL_NAME_GPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le7s4ioi35c9"
   },
   "source": [
    "One values that are not correct from the first table:\n",
    "\n",
    "\"2021\": ==> \"net_income\": 33.1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8cc0DsY-V-0"
   },
   "source": [
    "## Prompt: Markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF_cLnT-rX4c"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_png}\"}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract raw data from the image in a markdown format when it's possible.\"\n",
    "messages_gpt = append_message(content, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eNAOCFarGTl",
    "outputId": "1c555918-839d-4970-8237-b85670320c75"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL_NAME_GPT = \"gpt-4o\"\n",
    "print(get_completion_gpt4o(messages_gpt, MODEL_NAME_GPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGmR5Gd4rhwE"
   },
   "source": [
    "# GPT-4o-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teRjxX24rkxM"
   },
   "source": [
    "Not good: It extracted some of the data (from the chart) but the values are not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-UibB-eqFMt",
    "outputId": "a5d57845-7c68-4389-91db-b8fd25f1ed3d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL_NAME_GPT = \"gpt-4o-mini\"\n",
    "print(get_completion_gpt4o(messages_gpt, MODEL_NAME_GPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOM_2dqCpZlN",
    "outputId": "1326760b-f63e-481d-d62f-2496cf524642"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#I asked for markdown format\n",
    "MODEL_NAME_GPT = \"gpt-4o-mini\"\n",
    "print(get_completion_gpt4o(messages_gpt, MODEL_NAME_GPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsa3SBWZDTm3"
   },
   "source": [
    "# Claude Sonnet 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksCNzHpn-0m8"
   },
   "source": [
    "**Very Good results!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yk4FC7oT_Tft"
   },
   "outputs": [],
   "source": [
    "client_claude = anthropic.Anthropic(\n",
    "    api_key=CLAUDE_API_KEY,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
    "def get_completion_claude(messages):\n",
    "    response = client_claude.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2048,\n",
    "        temperature=0,\n",
    "        messages=messages\n",
    "    )\n",
    "    print(response.model)\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTe9WV0Vwa5R"
   },
   "source": [
    "## Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-ufIYDo_bh9"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": encoded_png}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract raw data information from the images.\"\n",
    "messages_claude = append_message(content, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SaVMDJCsrzbc",
    "outputId": "c0413be5-383b-489e-efca-99dd0df66fc2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
    "chart_analysis = get_completion_claude(messages_claude)\n",
    "print(chart_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "972TGMpI4bT8",
    "outputId": "f9132114-de2e-4004-d81b-4f06ac2f5f6a"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": encoded_png}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract raw data information from the images.\"\n",
    "messages_claude = append_message(content, question)\n",
    "\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
    "chart_analysis = get_completion_claude(messages_claude)\n",
    "print(chart_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QASjTySZwc7x"
   },
   "source": [
    "## Prompt 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHu5cLYws0h8"
   },
   "source": [
    "ðŸ”½ â¬‡ : In the following experiments, I asked Claude 3.5 Sonnet for markdwon format :\n",
    "\n",
    "1. It forgets about the first table ==> I only get the chart data\n",
    "==> It also stated that some data were in puropose not included for \"simplicity\"\n",
    "\n",
    "2. Then I modified the pormpt to explicitly ask it to gather **ALL** raw data from the image ==> It succedd then to gather all the numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT9siBfBr5E8"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": encoded_png}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract raw data information from the images in markdown format.\"\n",
    "messages_claude = append_message(content, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqXFfVD2stLt",
    "outputId": "981b9ffe-1652-4b20-e712-f814c99198a7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "chart_analysis = get_completion_claude(messages_claude)\n",
    "print(chart_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "281U-qwPwAqZ"
   },
   "source": [
    "ðŸ”½ â¬‡ \"Some data are not included in this table for simplicity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGEsYgyOuYqK",
    "outputId": "1b0cb0eb-bd8e-4675-f337-161667cbaa30"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": encoded_png}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract raw data from the images in a markdown format when it's possible.\"\n",
    "messages_claude = append_message(content, question)\n",
    "chart_analysis = get_completion_claude(messages_claude)\n",
    "print(chart_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BURy8vaYwg_B"
   },
   "source": [
    "## Prompt 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oX5Q1QNP-_5E"
   },
   "source": [
    "prompt with \"ALL\" explicitly set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5A1xVTqvCe7",
    "outputId": "07f8122d-7fa2-4ab1-bdd8-5e30a7e4dfa6"
   },
   "outputs": [],
   "source": [
    "content = [{\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/png\", \"data\": encoded_png}} for encoded_png in base64_encoded_pngs]\n",
    "question = \"Extract ALL raw data from the images in a markdown format when it's possible.\"\n",
    "messages_claude = append_message(content, question)\n",
    "chart_analysis = get_completion_claude(messages_claude)\n",
    "print(chart_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJAHiOduwnah"
   },
   "source": [
    "## Prompt 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmGg6WOdACXz",
    "outputId": "94f38c31-18d9-487b-e263-e1c6a2f969ab"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#\"Describe the image\" prompt\n",
    "chart_analysis = get_completion_claude(messages_claude)\n",
    "print(chart_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24n7YqfsFI6E"
   },
   "source": [
    "# Llama 3.2 11B - Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCf3QYu876aT"
   },
   "source": [
    "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehsBA2hu8fEC"
   },
   "source": [
    "## Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_DnoMhZEHr5"
   },
   "outputs": [],
   "source": [
    "# Install this to be able to use : MllamaForConditionalGeneration ==> A simple pip install does not work\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LkjsFxdBSoC"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94QZeYgQSwvr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaCissBZS0KT"
   },
   "outputs": [],
   "source": [
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, #auto\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "#processor.apply_chat_template  ==> does not work ==> it doesn't have chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OPYpm2aTLab"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(path_img)\n",
    "\n",
    "# prompt = \"<|image|><|begin_of_text|>Extract raw data information from the image.\"\n",
    "\n",
    "# <|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "# <|image|>Extract raw data information from the image:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "prompt=\"\"\"\n",
    "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "<|image|>Extract raw data information from the image:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHaBwqT-TNSb",
    "outputId": "6a5e16d1-cea0-4a4e-bf05-addbe9abd16a"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output = model.generate(**inputs)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fQv8oY2_RgT"
   },
   "source": [
    "Results: **I'm not able to provide that information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FY6sH3YlYdD_"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "##Inference time is so looonnng ==> stop it because it's not possible\n",
    "output = model.generate(**inputs, max_new_tokens = 2048)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjGkn-FY8qU6"
   },
   "source": [
    "## With Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVveRbSz_kPz"
   },
   "source": [
    "Not good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPCzr5bX9Q_h"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "# accelerate bitsandbytes huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "ca401e53b61d415ca807b55eecbccacf",
      "fcb8dd0f45be46418de77fad89858c3f",
      "364494863b3a4c3d992d6016e7819def",
      "73c0158f08e94397a82eecad45bdeac4",
      "d30f2ee0a90241719f2024c488fa0782",
      "ea7c9df9f52446b3a7a018b8bba675f6",
      "942ff7736ebc477e9357d08d53f07814",
      "9cf2387976334c97a12f691cb242df6c",
      "772c8ed505584bdfb44d8c7eb5bf0e34",
      "052cf1778d5440a08b11d16f0a40fd9b",
      "06be5f3716b2411080fa3e9b465bc89a"
     ]
    },
    "id": "eHMxWwB48ttx",
    "outputId": "6a1cf9fa-af04-4c52-b320-0ae820564d0e"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "model_qtz = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_W72x_UDDpoo"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(path_img)\n",
    "\n",
    "prompt=\"\"\"\n",
    "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "<|image|>Extract raw data information from the image:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model_qtz.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQlC1pR7Du_T",
    "outputId": "4e238ec7-a0bf-4846-b62c-48261eee4dea"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output = model_qtz.generate(**inputs, max_new_tokens = 2048)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX6PjgKnu-Ee"
   },
   "source": [
    "# Llama 3.2 11B Vision -Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hkd7OKFWDw2C"
   },
   "source": [
    "## With Quantization to 4-bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CYKYVoF_zNT"
   },
   "source": [
    "It extracted redundant information and numbers are inacurrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoozOrK-zFqb"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoR64Uh-vT0-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1eQ1mt6AG35"
   },
   "source": [
    "BitsAndBytesConfig is part of the effort to make transformer models more efficient by using quantization techniques, particularly when loading models with reduced precision, like 8-bit or 4-bit integer types, instead of the standard 32-bit floating-point numbers.\n",
    "\n",
    " The BitsAndBytesConfig allows you to configure how a model is loaded and run in lower precision.\n",
    "\n",
    "\n",
    "NF4 stands for \"Normalized Float 4-bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EauMCeLi0N5a",
    "outputId": "cf42618a-da4b-49ab-d03c-1264c5e7d129"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from PIL import Image\n",
    "image = Image.open(path_img)\n",
    "\n",
    "# prompt = \"<|image|><|begin_of_text|>Extract raw data information from the image.\"\n",
    "# inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"Extract raw data information from the image:\"}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(image, input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=2048)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ghchevFFELO"
   },
   "source": [
    "# Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjuJWTUO7_tT"
   },
   "source": [
    "https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMWmjvva3RNY"
   },
   "source": [
    "It successfully extracted the numbers from the chart and highlighted the various metrics. However, the numbers provided are inaccurate. There is also a discrepancy between the bar values (Net Income), which are extracted fairly accurately, and the line values (EPS and ROTCE), which are incorrect for nearly all years.\n",
    "However it didn't extract the first table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmRdWQ3F3K_B",
    "outputId": "d1a9a5a4-b2f7-4d13-b884-e419972fac69"
   },
   "outputs": [],
   "source": [
    "!pip install qwen_vl_utils -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ap693jMxAU9j"
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2alQU2bz39lZ",
    "outputId": "2bb32316-dcbd-453f-eb28-cf6ee92ad99b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": path_img,\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Extract raw data information from the image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=2048)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTaFh4pfDLsK"
   },
   "source": [
    "# Pixtral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGWJvWqP7yki"
   },
   "source": [
    "\n",
    "https://docs.mistral.ai/capabilities/vision/\n",
    "\n",
    "https://huggingface.co/mistralai/Pixtral-12B-2409"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c1VlunXmX_H"
   },
   "source": [
    "## MistralAI package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aG3I5GlwxfR"
   },
   "source": [
    "The model extraced information coming from the table and charts. It provides the different metrics included in the image.\n",
    "\n",
    "The table values are mostly correct. However, while some of the values in the charts are accurate, the majority are incorrect.\n",
    "\n",
    "I believe we can use it for image description, but when it comes to extracting numbers, we should rely on larger models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMXgGijgz6a3",
    "outputId": "1c211063-8612-4740-e073-756ca264cf5e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "\n",
    "base64_image = base64_encoded_pngs[0]\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "api_key = MISTRAL_API_KEY\n",
    "model = \"pixtral-12b-2409\"\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Extract raw data information from the charts.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": f\"data:image/png;base64,{base64_image}\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get the chat response\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "052cf1778d5440a08b11d16f0a40fd9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06be5f3716b2411080fa3e9b465bc89a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "364494863b3a4c3d992d6016e7819def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cf2387976334c97a12f691cb242df6c",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_772c8ed505584bdfb44d8c7eb5bf0e34",
      "value": 5
     }
    },
    "73c0158f08e94397a82eecad45bdeac4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_052cf1778d5440a08b11d16f0a40fd9b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_06be5f3716b2411080fa3e9b465bc89a",
      "value": "â€‡5/5â€‡[02:05&lt;00:00,â€‡22.03s/it]"
     }
    },
    "772c8ed505584bdfb44d8c7eb5bf0e34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "942ff7736ebc477e9357d08d53f07814": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cf2387976334c97a12f691cb242df6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca401e53b61d415ca807b55eecbccacf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fcb8dd0f45be46418de77fad89858c3f",
       "IPY_MODEL_364494863b3a4c3d992d6016e7819def",
       "IPY_MODEL_73c0158f08e94397a82eecad45bdeac4"
      ],
      "layout": "IPY_MODEL_d30f2ee0a90241719f2024c488fa0782"
     }
    },
    "d30f2ee0a90241719f2024c488fa0782": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea7c9df9f52446b3a7a018b8bba675f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcb8dd0f45be46418de77fad89858c3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea7c9df9f52446b3a7a018b8bba675f6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_942ff7736ebc477e9357d08d53f07814",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
