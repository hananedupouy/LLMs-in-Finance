{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YVPjLN9cVAU"
   },
   "source": [
    "**How to use Query transformation and Hypothetical answering to re-rank retrieved articles and enhance the performance of your RAG pipeline?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjEguVl88FjA"
   },
   "source": [
    "[Hanane DUPOUY LinkedIn](https://https://www.linkedin.com/in/hanane-d-algo-trader/): https://www.linkedin.com/in/hanane-d-algo-trader/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziSY8wNYkHLQ"
   },
   "source": [
    "Ever wondered how to accurately answer the question, `'What impact did the global outage of CrowdStrike, extensively used by Microsoft, have on Microsoft's stock price?'`\n",
    "\n",
    "To tackle this, we'll employ the **hypothetical answer re-ranking** technique:\n",
    "\n",
    "\n",
    "-**GOAL**: I'll evaluate whether the **hypothetical answer** can improve the re-ranking and the retrieved context, and subsequently enhance the LLM's response within our RAG pipeline. Alternatively, we will assess if the original query alone is sufficient to retrieve the appropriate context and deliver accurate results.\n",
    "\n",
    "- Different techniques will be employed: **Query Transformation, Hypothetical Answers, Embeddings, and Similarity Scoring** to retrieve the relevant context from news articles fetched from the NEWS API.\n",
    "\n",
    "\n",
    "- In these techniques, we will compare the **capabilities of three LLMs**: **gpt-4o-mini** (the latest small model from OpenAI), **gpt-4o** (the most capable LLM from OpenAI), and **gpt-3.5-turbo**.\"\n",
    "\n",
    "\n",
    "- I'll use use **3 evaluation metrics** from **deepEval** for RAG pipelines: **Faithfulness, Context Relevancy and Answer relevancy**.\n",
    "These metrics are explained in the notebook.\n",
    "\n",
    "- We will compare the three LLMs using two techniques: **Hypothetical Answer Re-ranking vs. Original Query Retrieval**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thVYKZfbfkFy"
   },
   "source": [
    "**Steps:**\n",
    "\n",
    "For each of the three LLMs: **GPT-4o-mini, GPT-4o, and GPT-3.5-turbo**:\n",
    "\n",
    "**1-** We will perform **Search Queries** (or **Query transformation**) using the LLM to generate various formulation with the same keywords from the original user query.\n",
    "\n",
    "**2-** We will use an LLM to generate a **hypothetical answer**. This creative response will serve as a potential answer, using placeholders instead of actual facts.\n",
    "\n",
    "**3- **Based on each query from the search queries (1-), we will retrieve news article from NEWS API.\n",
    "\n",
    "**4-** We will **embedd** user query, hypothetical answer and the collected articles\n",
    "\n",
    "**5-** We compute the **similarity score** between 2 sets:\n",
    "\n",
    "  5-1- Hypothetical answer (2-) vs retrieved context (3-)\n",
    "\n",
    "  5-1- Original query (2-) vs retrieved context (3-)\n",
    "\n",
    "**6- **Ask the LLM to give the final answer based on the user query and the retrieved context\n",
    "\n",
    "**7-** Use 3 evalutaions metrics from DeepVal to evaluate the RAG pipeline: **Faithfulness, Context Relevancy and Answer relevancy.**\n",
    "\n",
    "**8-** Key Takeways\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSDlVKdkwRfZ"
   },
   "source": [
    "# Install Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sR9R3NACmxls"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Xl1_XIbmm1D"
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "from google.colab import userdata\n",
    "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
    "news_api_key = userdata.get('NEWS_API_KEY')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynq6pzcuwT64"
   },
   "source": [
    "Chat method OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDXktg4Bmt9U"
   },
   "outputs": [],
   "source": [
    "def get_completion_gpt(input, gpt_model = \"gpt-3.5-turbo\"):\n",
    "  completion = client.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Output only valid JSON\"},\n",
    "            {\"role\": \"user\", \"content\": input},\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "\n",
    "  text = completion.choices[0].message.content\n",
    "  parsed = json.loads(text)\n",
    "  return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paqVeXGKy6H3"
   },
   "source": [
    "# Search NEWS API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRS4dKWYy4Ww"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def search_news(query, news_api_key= news_api_key,num_articles=5, from_datetime = \"2024-07-18\",to_datetime = \"2024-07-21\"):\n",
    "    response = requests.get(\n",
    "        \"https://newsapi.org/v2/everything\",\n",
    "        params={\n",
    "            \"q\": query,\n",
    "            \"apiKey\": news_api_key,\n",
    "            \"pageSize\": num_articles,\n",
    "            \"sortBy\": \"relevancy\",\n",
    "            \"from\": from_datetime,\n",
    "            \"to\": to_datetime,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60L1oojrwbeI"
   },
   "source": [
    "# Generate Search Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6K5h05Dm8cL",
    "outputId": "69c1e753-4bc2-4964-f590-f8be0d66f015"
   },
   "outputs": [],
   "source": [
    "user_query = \"What impact did the global outage of CrowdStrike, which is used extensively by Microsoft, have on Microsoft's stock price?\"\n",
    "\n",
    "input = f\"\"\"\n",
    "You have access to a NEWS API that returns recent news articles related to the user's question.\n",
    "\n",
    "1. Make a list of search queries that match the topic described in the user's question.\n",
    "2. Use different keywords related to the topic to create a variety of queries, making some general and others more specific.\n",
    "3. Be imaginative and generate as many queries as possible. More queries will help you find better results.\n",
    "4. Pick 10 of these queries.\n",
    "For example, you can include queries like ['keyword_1 keyword_2', 'keyword_1', 'keyword_2'].\n",
    "\n",
    "# User question: {user_query}\n",
    "\n",
    "# Format: {{\"queries\": [\"query_1\", \"query_2\", \"query_3\"]}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llms = [\"gpt-3.5-turbo\", \"gpt-4o-mini\", \"gpt-4o\"]\n",
    "\n",
    "dict_questions = {}\n",
    "for llm in llms:\n",
    "  print(llm)\n",
    "  parsed = get_completion_gpt(input, gpt_model = llm)\n",
    "  dict_questions[llm] = parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqwOXh1SqQbI",
    "outputId": "e6b924c9-ad5b-494d-d617-752a39abd491"
   },
   "outputs": [],
   "source": [
    "dict_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDbVHrB8xUTR"
   },
   "source": [
    "# Create a Hypothetical answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByTLCPEYxW9o",
    "outputId": "fa08ce77-add9-43c7-9994-7cb4b781c401"
   },
   "outputs": [],
   "source": [
    "hypoth_answer = f\"\"\"\n",
    "Make up an answer to the user's question. We'll use this fabricated answer to sort the search results.\n",
    "Imagine you have all the details to answer, but don't use real facts. Do not give any numbers.\n",
    "Instead, use placeholders like 'EVENT affected something,' 'NAME mentioned something on DATE,' or 'EVENT has caused something.'\n",
    "\n",
    "User question: {user_query}\n",
    "\n",
    "Format: {{\"hypotheticalAnswer\": \"hypothetical answer text\"}}\n",
    "\"\"\"\n",
    "\n",
    "print(hypoth_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DI5PoRbXxq84",
    "outputId": "2969da82-f1a0-40c8-fefa-ed02c1bf8289"
   },
   "outputs": [],
   "source": [
    "#Trying differenet llms:\n",
    "\n",
    "hypoth_answer_llms = {}\n",
    "for llm in llms:\n",
    "  # print(llm)\n",
    "  parsed_hypothet_answer = get_completion_gpt(hypoth_answer, gpt_model = llm)\n",
    "  hypoth_answer_llms[llm] = parsed_hypothet_answer['hypotheticalAnswer']\n",
    "  print(f\"{llm}\\n {hypoth_answer_llms[llm]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkzeKQEnqohC"
   },
   "source": [
    "# Fetch news articles from NEWS API for each query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsusZSF9zX3p"
   },
   "outputs": [],
   "source": [
    "def get_articles_from_news_api(queries):\n",
    "  articles = []\n",
    "  for query in queries:\n",
    "    result = search_news(query)\n",
    "    if result['status'] == 'ok':\n",
    "      articles = articles + result['articles']\n",
    "    else:\n",
    "      raise Exception(result[\"message\"])\n",
    "  return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1puWVqszD4Q"
   },
   "outputs": [],
   "source": [
    "articles={}\n",
    "for llm in llms:\n",
    "  queries = dict_questions[llm]['queries']\n",
    "  queries.append(user_query)\n",
    "  articles[llm] =  get_articles_from_news_api(queries)\n",
    "  if articles[llm]!=None:\n",
    "    articles[llm] = list({article[\"url\"]: article for article in articles[llm]}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0Dw8l5d0Pv3",
    "outputId": "e1e55e9e-e833-4e0c-c075-c0aae5f8c01b"
   },
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "  print(len(articles[llm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faIMg0BtUbn2"
   },
   "outputs": [],
   "source": [
    "# #To save data locally\n",
    "# for llm in llms:\n",
    "#   pd.DataFrame(articles[llm]).to_csv(\"articles_\"+llm+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3uy09GTrHkq",
    "outputId": "9a163b92-96af-4c91-8c03-d4b2d8a305bb"
   },
   "outputs": [],
   "source": [
    "#Display some articles:\n",
    "print(\"Total number of articles:\", len(articles)) #3 LLM ==> 3 set of articles\n",
    "llm = llms[-1]\n",
    "for article in articles[llm][0:5]:\n",
    "    print(\"Title:\", article[\"title\"])\n",
    "    print(\"Url:\", article[\"url\"])\n",
    "    print(\"Description:\", article[\"description\"])\n",
    "    print(\"Content:\", article[\"content\"][0:300] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ-R5HO80ptu"
   },
   "source": [
    "# Embeddings and cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k3iEBkn0wH-"
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1_t7_v9sdgL"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(input):\n",
    "    response = client.embeddings.create(model=\"text-embedding-ada-002\", input=input)\n",
    "    return [data.embedding for data in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNs44lx501Ls"
   },
   "outputs": [],
   "source": [
    "def get_embeddings_articles(articles):\n",
    "  articles_prepare_embedd =  [\n",
    "        f\"{article['title']} {article['description']} {article['content'][0:700]}\"\n",
    "        for article in articles\n",
    "    ]\n",
    "\n",
    "  print(f\"Length of articles to embed: {len(articles_prepare_embedd)}\")\n",
    "  article_embeddings = get_embeddings(articles_prepare_embedd)\n",
    "  return article_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II7JHwgc2FEJ"
   },
   "outputs": [],
   "source": [
    "similarity_score_func=lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "\n",
    "def calculate_cosine_distance(embedding_hypoth, article_embeddings):\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for article_embedding in article_embeddings:\n",
    "        cosine_similarities.append(similarity_score_func(embedding_hypoth, article_embedding))\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aa11gCV3cr7"
   },
   "outputs": [],
   "source": [
    "def sort_articles_by_cosine_similarity(articles, cosine_similarities):\n",
    "    scored_articles = zip(articles, cosine_similarities)\n",
    "    sorted_articles = sorted(scored_articles, key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Top 5 articles scores: {[score for _,score in sorted_articles[0:5]]}\\n\")\n",
    "    # for article, score in sorted_articles[0:5]:\n",
    "    #     print(\"Title:\", article[\"title\"])\n",
    "    #     # print(\"Url:\", article[\"url\"])\n",
    "    #     # print(\"Date of publication:\", article[\"publishedAt\"])\n",
    "    #     # print(\"Description:\", article[\"description\"])\n",
    "    #     # print(\"Content:\", article[\"content\"][0:50] + \"...\")\n",
    "    #     print(\"Score:\", score)\n",
    "    #     print()\n",
    "    return sorted_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4aCEXg76BF3"
   },
   "outputs": [],
   "source": [
    "def context_retrieval(sorted_articles):\n",
    "    \"\"\"Get top 5 articles based on their similarity scores.\"\"\"\n",
    "    formatted_top_results = [article[\"title\"]+\"\\n\"+article[\"description\"]+\"\\n\"+article[\"content\"] for article, _score in sorted_articles[0:5]]\n",
    "\n",
    "    return formatted_top_results\n",
    "\n",
    "\n",
    "def get_final_answer(user_query, formatted_top_results, llm):\n",
    "  \"\"\"Answer the user's question based on the retrieved context using a GPT model: gpt-4o, gpt-4o-mini, gpt-3.5-turbo.\"\"\"\n",
    "  final_input = f\"\"\"\n",
    "  Generate an answer to the user's question based on the given search results.\n",
    "  TOP_RESULTS: {formatted_top_results}\n",
    "  USER_QUESTION: {user_query}\n",
    "\n",
    "  Include as much information as possible in the answer. Reference the relevant search result urls as markdown links.\n",
    "  \"\"\"\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "          model=llm,\n",
    "          messages=[\n",
    "              {\"role\": \"user\", \"content\": final_input},\n",
    "          ],\n",
    "      )\n",
    "\n",
    "  return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vlu3M_Vt9Xb9"
   },
   "source": [
    "# Example 1 LLM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00NzHPet5Q-9"
   },
   "source": [
    "## Similarities against the Hypothetical Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Lis1URAsf1h",
    "outputId": "16c79efd-45bb-4643-dd43-a1d8b0853fc2"
   },
   "outputs": [],
   "source": [
    "llm = llms[0]\n",
    "embedding_hypoth = get_embeddings(hypoth_answer_llms[llm])[0]\n",
    "article_embeddings = get_embeddings_articles(articles[llm]) #{list of embedded articles , there are 26 articles}\n",
    "cosine_similarities_hypoth = calculate_cosine_distance(embedding_hypoth, article_embeddings)\n",
    "\n",
    "print(f\" len embedding vector={len(embedding_hypoth)}, len artciles embedded={len(article_embeddings)}, len cosine_distance vector={len(cosine_similarities_hypoth)}\")\n",
    "print(cosine_similarities_hypoth[:5])\n",
    "print(\"\\n\")\n",
    "\n",
    "sorted_articles_hypoth = sort_articles_by_cosine_similarity(articles[llm], cosine_similarities_hypoth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bTzw0OS9br7"
   },
   "source": [
    "## Similarities against the original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROlTBfXE5pwe",
    "outputId": "b195988b-f9ef-4794-9d46-401cabde4270"
   },
   "outputs": [],
   "source": [
    "llm = llms[0]\n",
    "embedding_original_query = get_embeddings(user_query)[0]\n",
    "# article_embeddings = get_embeddings_articles(articles[llm]) #already embedded in the cell before\n",
    "cosine_similarities_original= calculate_cosine_distance(embedding_original_query, article_embeddings)\n",
    "\n",
    "print(f\" len embedding vector={len(embedding_original_query)}, len artciles embedded={len(article_embeddings)}, len cosine_distance vector={len(cosine_similarities_original)}\")\n",
    "print(cosine_similarities_original[:5])\n",
    "print(\"\\n\")\n",
    "\n",
    "sorted_articles_original = sort_articles_by_cosine_similarity(articles[llm], cosine_similarities_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwA7lXS-vNxZ"
   },
   "source": [
    "## Final Answer: Calling LLM to answer the user query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj_v-0wK7sv1"
   },
   "source": [
    "### Against the hypothetical answer and the original user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "SQxNzWaz6Xmd",
    "outputId": "a6dbefa8-5330-409f-b955-85f3e84f8531"
   },
   "outputs": [],
   "source": [
    "#Using the retrieved context coming from the hypothetical answer\n",
    "formatted_top_results_hypoth = context_retrieval(sorted_articles_hypoth)\n",
    "final_answer_hypoth = get_final_answer(user_query, formatted_top_results_hypoth, llm)\n",
    "print(\"Final answer against the Hypothetical query\")\n",
    "display.display(display.Markdown(final_answer_hypoth))\n",
    "\n",
    "#Using the retrieved context coming from the original answer\n",
    "formatted_top_results_original = context_retrieval(sorted_articles_original)\n",
    "final_answer_original = get_final_answer(user_query, formatted_top_results_original, llm)\n",
    "print(\"Final answer against the original query\")\n",
    "display.display(display.Markdown(final_answer_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dKxtKHx7zGM"
   },
   "source": [
    "# All together: With the 3 LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWKj4HjdaDEz",
    "outputId": "857c0c2d-ab23-41b0-de29-7e8b2c998c5f"
   },
   "outputs": [],
   "source": [
    "llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MJXRI6c77yLb",
    "outputId": "47af3d4f-8ab2-4d5b-c402-45b7b8013d59"
   },
   "outputs": [],
   "source": [
    "embedding_original_query = get_embeddings(user_query)[0]\n",
    "\n",
    "for llm in llms:\n",
    "  print(llm)\n",
    "  #Hypothetical answer\n",
    "  embedding_hypoth = get_embeddings(hypoth_answer_llms[llm])[0]\n",
    "  article_embeddings = get_embeddings_articles(articles[llm])\n",
    "  cosine_similarities_hypoth = calculate_cosine_distance(embedding_hypoth, article_embeddings)\n",
    "  print(\"Hypothetical Answer: Most relevant News\\n\")\n",
    "  sorted_articles_hypoth = sort_articles_by_cosine_similarity(articles[llm], cosine_similarities_hypoth)\n",
    "  print(\"-\"*50)\n",
    "\n",
    "  #Original Query\n",
    "  cosine_similarities_original= calculate_cosine_distance(embedding_original_query, article_embeddings)\n",
    "  print(\"Original Answer: Most relevant News\\n\")\n",
    "  sorted_articles_original = sort_articles_by_cosine_similarity(articles[llm], cosine_similarities_original)\n",
    "  print(\"-\"*50)\n",
    "\n",
    "  formatted_top_results_hypoth = context_retrieval(sorted_articles_hypoth)\n",
    "  final_answer_hypoth = get_final_answer(user_query, formatted_top_results_hypoth, llm)\n",
    "  print(\"Final answer against the Hypothetical query\")\n",
    "  display.display(display.Markdown(final_answer_hypoth))\n",
    "  print(\"-\"*50)\n",
    "\n",
    "  formatted_top_results_original = context_retrieval(sorted_articles_original)\n",
    "  final_answer_original = get_final_answer(user_query, formatted_top_results_original, llm)\n",
    "  print(\"Final answer against the original query\")\n",
    "  display.display(display.Markdown(final_answer_original))\n",
    "  print(\"-\"*50)\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BBJNRaO4_ug8",
    "outputId": "b4682a29-f291-4e45-f62e-57e35f36dd01"
   },
   "outputs": [],
   "source": [
    "len(sorted_articles_hypoth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0k9PzgPDspK"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wmglABcGerD"
   },
   "source": [
    "We will be using DeepEval, to compute 3 metrics:\n",
    "\n",
    "*   Faithfulness\n",
    "*   Context Relevancy\n",
    "*   Anwser Relevancy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI1RKr4R_atT"
   },
   "outputs": [],
   "source": [
    "!pip install deepeval -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_UUi0NX-lz1"
   },
   "source": [
    "You need to speficy your OpenAI API key to use DeepEval, in our case.\n",
    "\n",
    "To compute metrics, this library makes several calls to a given LLM, per default they are using GPT-4o.\n",
    "You can use a custom LLM if you want.\n",
    "\n",
    "However note that the under-hood pormpt templates, in the metrics, the LLM is asked to outpout a json format, if you are using a small LLM, this part may not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymxOJWylBI3C"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "import os\n",
    "os.environ[ \"OPENAI_API_KEY\" ] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71SB9xe_DlcR"
   },
   "source": [
    "## Faithfullness: Retrieved Context vs LLM's final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49J-NWgautB7"
   },
   "source": [
    "This evaluates the factual consistency of the **generated answer** relative to the **provided context**.\n",
    "\n",
    "it outputs a **reason** for its **metric score**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvq1jDW3uzFc"
   },
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKnFtge4uv5Z"
   },
   "source": [
    "1- Use an LLM to break it into statements\n",
    "\n",
    "2- Using an LLM, assert if the statement can or not be inferred from the context ⇒ Verdict: yes or no or idk.\n",
    "\n",
    "3- Compute Faithfulness Score:\n",
    "\n",
    "Faithfulness= Number of Truthful Claims/Total Number of Claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXkaeoJj_sAt"
   },
   "source": [
    "https://docs.confident-ai.com/docs/metrics-faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBMy7CCH_Ys0"
   },
   "outputs": [],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PnE4gVEFLm6"
   },
   "outputs": [],
   "source": [
    "def get_faithfulness_metric(user_query,final_answer,formatted_top_results):\n",
    "  metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    "  )\n",
    "  test_case = LLMTestCase(\n",
    "      input= user_query,\n",
    "      actual_output=final_answer,\n",
    "      retrieval_context=formatted_top_results\n",
    "  )\n",
    "\n",
    "  metric.measure(test_case)\n",
    "  score = metric.score\n",
    "  reason = metric.reason\n",
    "  return score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kscu4gznDyfJ"
   },
   "source": [
    "### Original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89,
     "referenced_widgets": [
      "99c810c345144611a574b0354978952d",
      "9beb2254b21c408c905f63b635acef79"
     ]
    },
    "id": "w5gf8aSm_oVj",
    "outputId": "d1aa7af7-ff09-433e-f951-31556ba0b5d0"
   },
   "outputs": [],
   "source": [
    "metric = FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=user_query,\n",
    "    actual_output=final_answer_original,\n",
    "    retrieval_context=formatted_top_results_original\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbwnfeSAD1SB"
   },
   "source": [
    "### Hypothetical answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpE_wywQFtta"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEHn1tL3-P62"
   },
   "outputs": [],
   "source": [
    "score, reason = get_faithfulness_metric(user_query,final_answer_hypoth,formatted_top_results_hypoth)\n",
    "print(f\"Score: {score}, Reason: {reason}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "e1228e602142478db648aad0ff915991",
      "795101f3f9d242fd9633bbbc8f36f08e"
     ]
    },
    "id": "s7ATKMLqFYFY",
    "outputId": "226adfd0-3ebd-4851-ee69-b3c3cf3cdd26"
   },
   "outputs": [],
   "source": [
    "score, reason = get_faithfulness_metric(user_query,final_answer_original,formatted_top_results_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJaOUASBEXmx"
   },
   "source": [
    "## Context Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtgb4qA8EF6a"
   },
   "source": [
    "This evaluates how relevant the **retrieved context** is to the **input query**.\n",
    "\n",
    "It outputs a **reason** for its **metric score**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUb4FG5XvdmB"
   },
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHRVxM6fvapB"
   },
   "source": [
    "1- Use an LLM to extract statements from the retrieved context\n",
    "\n",
    "2- Using an LLM, assert if each statement is relevant to the input query ==>  yes or no.\n",
    "\n",
    "3- Compute Contextual Relevancy Score:\n",
    "\n",
    "Contextual Relevancy=\n",
    "Number of Relevant Statements/Total Number of Statements​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPEg8oo-_uvN"
   },
   "source": [
    "https://docs.confident-ai.com/docs/metrics-contextual-relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNNNeF8fD4-x"
   },
   "outputs": [],
   "source": [
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "# from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBZOXyf1En4J"
   },
   "outputs": [],
   "source": [
    "def get_context_relevancy_metric(user_query,final_answer_hypoth,formatted_top_results_hypoth):\n",
    "  metric = ContextualRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    "  )\n",
    "  test_case = LLMTestCase(\n",
    "      input= user_query,\n",
    "      actual_output=final_answer_hypoth,\n",
    "      retrieval_context=formatted_top_results_hypoth\n",
    "  )\n",
    "\n",
    "  metric.measure(test_case)\n",
    "  score = metric.score\n",
    "  reason = metric.reason\n",
    "  return score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "touFdqajX67L"
   },
   "source": [
    "### Original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70,
     "referenced_widgets": [
      "7cbc0bc6cd564bc1b8153b6ec1742cd4",
      "017397fb1d264a55a5aff42cd2600226"
     ]
    },
    "id": "YSkvQICsFDhq",
    "outputId": "7c045b25-3a2a-48d5-8c3e-b1664a237f76"
   },
   "outputs": [],
   "source": [
    "score, reason = get_context_relevancy_metric(user_query,final_answer_original,formatted_top_results_original)\n",
    "print(f\"Score: {score}, Reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li6tSnbNX_wL"
   },
   "source": [
    "### Hypothetical answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "referenced_widgets": [
      "8afc66dbf89641aba9f4041b8b334eee",
      "90b6385e7acb42878af9995a46789a2f"
     ]
    },
    "id": "N23_zB56E_6o",
    "outputId": "3375bf24-d040-46cb-eeed-aa5168146db0"
   },
   "outputs": [],
   "source": [
    "score, reason = get_context_relevancy_metric(user_query,final_answer_hypoth,formatted_top_results_hypoth)\n",
    "print(f\"Score: {score}, Reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU7GNmmcF9g5"
   },
   "source": [
    "## Answer Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UuzUkEsu90S"
   },
   "source": [
    "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the **actual_output** (final answer) of your LLM application is compared to the provided **input**.\n",
    "\n",
    "\n",
    "deepeval's answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a **reason** for its **metric score**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxqkMX7y_yi-"
   },
   "source": [
    "https://docs.confident-ai.com/docs/metrics-answer-relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvW1NdwBF-Po"
   },
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfEPq2YAGF0p"
   },
   "outputs": [],
   "source": [
    "def get_answer_relevancy_metric(user_query,final_answer_hypoth):\n",
    "  metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o\",\n",
    "    include_reason=True\n",
    "  )\n",
    "  test_case = LLMTestCase(\n",
    "      input= user_query,\n",
    "      actual_output=final_answer_hypoth,\n",
    "      # retrieval_context=formatted_top_results_hypoth\n",
    "  )\n",
    "\n",
    "  metric.measure(test_case)\n",
    "  score = metric.score\n",
    "  reason = metric.reason\n",
    "  return score, reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_I6JLZ7YHii"
   },
   "source": [
    "### Original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "referenced_widgets": [
      "9ae13990f4b94589bedfecf87ccfae38",
      "569d02b2e1b14d639d0341fb1cb96e8f"
     ]
    },
    "id": "yR6iyT8aGObp",
    "outputId": "88c71f1d-cd4a-43cd-d1df-aba922404353"
   },
   "outputs": [],
   "source": [
    "score, reason = get_answer_relevancy_metric(user_query,final_answer_original)\n",
    "print(f\"Score: {score}, Reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AL18Wy2YKzb"
   },
   "source": [
    "### Hypothetical answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "referenced_widgets": [
      "b7e41a0b97244238ad271140f9b7755e",
      "029cbff8d3ab423aaf51d2a62f285bf1"
     ]
    },
    "id": "sJ-RgJYJYB5S",
    "outputId": "98abac39-d124-4ee8-b87d-fc1bac5a7bf1"
   },
   "outputs": [],
   "source": [
    "score, reason = get_answer_relevancy_metric(user_query,final_answer_hypoth)\n",
    "print(f\"Score: {score}, Reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91Otg8-XG9CJ"
   },
   "source": [
    "# All together: LLMs + Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8O7NUFOaR02W"
   },
   "source": [
    "## Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjFbAV9XHPdA"
   },
   "outputs": [],
   "source": [
    "def get_all_eval_metrics(user_query,final_answer,formatted_top_results):\n",
    "  score_faithfulness, reason_faithfulness = get_faithfulness_metric(user_query,final_answer,formatted_top_results)\n",
    "  score_cxt_relev, reason_cxt_relev = get_context_relevancy_metric(user_query,final_answer,formatted_top_results)\n",
    "  score_answ_relev, reason_answ_relev = get_answer_relevancy_metric(user_query,final_answer)\n",
    "  print(\"\\nFaithfulness\\n\")\n",
    "  print(score_faithfulness, reason_faithfulness)\n",
    "  print(\"\\nContext Relevancy\\n\")\n",
    "  print(score_cxt_relev, reason_cxt_relev)\n",
    "  print(\"\\nAnswer Relevancy\\n\")\n",
    "  print(score_answ_relev, reason_answ_relev)\n",
    "\n",
    "  return score_faithfulness, reason_faithfulness, score_cxt_relev, reason_cxt_relev, score_answ_relev, reason_answ_relev\n",
    "    # return (score_faithfulness, reason_faithfulness), (score_cxt_relev, reason_cxt_relev), (score_answ_relev, reason_answ_relev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xtCwoYJG8Tw"
   },
   "outputs": [],
   "source": [
    "# llm = llms[0]\n",
    "\n",
    "embedding_original_query = get_embeddings(user_query)[0]\n",
    "scores_hypoth = {}\n",
    "scores_original = {}\n",
    "\n",
    "for llm in llms:\n",
    "  print(llm)\n",
    "  #Hypothetical answer\n",
    "  embedding_hypoth = get_embeddings(hypoth_answer_llms[llm])[0]\n",
    "  article_embeddings = get_embeddings_articles(articles[llm]) #{list of embedded articles , there are 26 articles}\n",
    "  cosine_similarities_hypoth = calculate_cosine_distance(embedding_hypoth, article_embeddings)\n",
    "  print(\"Hypothetical Answer: Most relevant News\\n\")\n",
    "  sorted_articles_hypoth = sort_articles_by_cosine_similarity(articles[llm], cosine_similarities_hypoth)\n",
    "  print(\"-\"*50)\n",
    "\n",
    "  #Original Query\n",
    "  cosine_similarities_original= calculate_cosine_distance(embedding_original_query, article_embeddings)\n",
    "  print(\"Original Answer: Most relevant News\\n\")\n",
    "  sorted_articles_original = sort_articles_by_cosine_similarity(articles[llm], cosine_similarities_original)\n",
    "  print(\"-\"*50)\n",
    "\n",
    "  formatted_top_results_hypoth = context_retrieval(sorted_articles_hypoth)\n",
    "  final_answer_hypoth = get_final_answer(user_query, formatted_top_results_hypoth, llm)\n",
    "  print(\"#Final answer against the Hypothetical query\")\n",
    "  display.display(display.Markdown(final_answer_hypoth))\n",
    "  print(\"-\"*50)\n",
    "\n",
    "  #Get Evaluations Metrics\n",
    "  scores_hypoth[llm] = get_all_eval_metrics(user_query,final_answer_hypoth,formatted_top_results_hypoth)\n",
    "\n",
    "  formatted_top_results_original = context_retrieval(sorted_articles_original)\n",
    "  final_answer_original = get_final_answer(user_query, formatted_top_results_original, llm)\n",
    "  print(\"#Final answer against the original query\")\n",
    "  display.display(display.Markdown(final_answer_original))\n",
    "\n",
    "  #Get Evaluations Metrics\n",
    "  scores_original[llm] = get_all_eval_metrics(user_query,final_answer_original,formatted_top_results_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp1K7q0aRyX-"
   },
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Hb00EJaLJAH",
    "outputId": "4d5f3c45-9a1e-45d9-b406-618bf8c80e2d"
   },
   "outputs": [],
   "source": [
    "scores_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAixna3aXW5R",
    "outputId": "461555e1-861c-4a76-a81a-d6b08882fb3e"
   },
   "outputs": [],
   "source": [
    "scores_hypoth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iWT2KwvLcSv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq05prDTXtA7"
   },
   "source": [
    "### Against Original Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "1lo6yWbILYGH",
    "outputId": "3fa51002-07c0-403b-f504-f93b90f0eb08"
   },
   "outputs": [],
   "source": [
    "scores_original_values = {}\n",
    "for llm in llms:\n",
    "  scores = [score for score in scores_original[llm] if type(score)!= str]\n",
    "  scores_original_values[llm] = scores\n",
    "\n",
    "pd.DataFrame(scores_original_values, index=['faithfulness','context_relevancy','answer_relevancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "JcGeWIAbgO8I",
    "outputId": "230d8a78-4b90-492e-9c2a-76a1be551171"
   },
   "outputs": [],
   "source": [
    "scores_original_raisons = {}\n",
    "for llm in llms:\n",
    "  raisons = [score for score in scores_original[llm] if type(score)== str]\n",
    "  scores_original_raisons[llm] = raisons\n",
    "\n",
    "pd.DataFrame(scores_original_raisons, index=['faithfulness','context_relevancy','answer_relevancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvAD0krbglsB",
    "outputId": "9ddfa359-bffd-4e50-81c0-87e3613b6f38"
   },
   "outputs": [],
   "source": [
    "index_metrics=['faithfulness','context_relevancy','answer_relevancy']\n",
    "for llm in llms:\n",
    "  print(f\"#{llm}:\")\n",
    "  for i in range(len(index_metrics)):\n",
    "    print(f\"{index_metrics[i]}\")\n",
    "    print(scores_original_raisons[llm][i])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIKiWNlBU_kC",
    "outputId": "0972d793-cc38-42de-ccc4-8d5d944cc2fd"
   },
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "  mean_score = np.mean([score for score in scores_original[llm] if type(score)!= str])\n",
    "  print(f\"{round(mean_score,3)} = Mean score for {llm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltSMvndEYWSL"
   },
   "source": [
    "**Key Takeaway 1**: gpt-4o-mini shows the best score among the other LLMs. Its score in answer relevancy was better than the one from gpt-4o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-xZRoTyYTQy"
   },
   "source": [
    "### Against Hypothetical Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "RxHGR6kRXJTU",
    "outputId": "5b883cc0-e4f7-4cac-81ac-5062e13ba330"
   },
   "outputs": [],
   "source": [
    "scores_hypoth_values = {}\n",
    "for llm in llms:\n",
    "  scores = [score for score in scores_hypoth[llm] if type(score)!= str]\n",
    "  scores_hypoth_values[llm] = scores\n",
    "\n",
    "pd.DataFrame(scores_hypoth_values, index=['faithfulness','context_relevancy','answer_relevancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "UJwqwI-PhuaQ",
    "outputId": "fd8bb50b-f497-4597-bc5b-d0c47e6679e2"
   },
   "outputs": [],
   "source": [
    "scores_hypoth_raisons = {}\n",
    "for llm in llms:\n",
    "  raisons = [score for score in scores_hypoth[llm] if type(score)== str]\n",
    "  scores_hypoth_raisons[llm] = raisons\n",
    "\n",
    "pd.DataFrame(scores_hypoth_raisons, index=['faithfulness','context_relevancy','answer_relevancy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNX2r_Bsh49X",
    "outputId": "9f4aacf4-fc9e-4135-d5a4-4f231308c0dd"
   },
   "outputs": [],
   "source": [
    "index_metrics=['faithfulness','context_relevancy','answer_relevancy']\n",
    "for llm in llms:\n",
    "  print(f\"#{llm}:\")\n",
    "  for i in range(len(index_metrics)):\n",
    "    print(f\"{index_metrics[i]}\")\n",
    "    print(scores_hypoth_raisons[llm][i])\n",
    "  print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mp3Pa_E3WWIb",
    "outputId": "20782138-955c-458e-ef0c-b9495e94d1a0"
   },
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "  mean_score = np.mean([score for score in scores_hypoth[llm] if type(score)!= str])\n",
    "  print(f\"{round(mean_score,3)} = Mean score for {llm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyDJcF_rYnEa"
   },
   "source": [
    "**Key Takeaway 2**: Again, in the hypothetical answer, GPT-4o-mini shows the best score among the other LLMs. Its score in answer relevancy was significantly better than GPT-4o (0.85 vs. 0.5) and even better than GPT-3.5-turbo (0.3). Furthermore, its score in faithfulness was better than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7Y4C_CrZSWi"
   },
   "source": [
    "**Key Takeaway 3** : Another important takeway:\n",
    "\n",
    "The score of the results coming from retrieval based on the hypotethical answer (0.619) is better than the one where retrieval is based on the original query (0.583), when using gpt-4o-mini. This higlights the fact that the re-ranking process leads to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRErXfZvaeYR"
   },
   "source": [
    "**Next:**\n",
    "\n",
    "**Retrieval Context:**\n",
    "\n",
    "Even if gpt-4o-mini is showing a good performance, however, the context relevance metric is 0 for all LLMs. This part needs to be reworked again.\n",
    "In the retrieval part, I took title + description and the beginning of the content. That was not enough.\n",
    "A good way needs to be : Parsing the whole html for each article, and gathering all this information together, chunking it in a given size\n",
    "\n",
    "**DeepEval and gpt-4o-mini:**\n",
    "\n",
    "It could be interesting to run evaluation metrics with gpt-4o-mini instead of gpt-40. Because the underhood calculation of the scores in the evaluation metrics are based on templated prompts and the capability of the LLM to well compare a given claim/statement in the retrieved context (for example) vs the final answer.\n",
    "\n",
    "This leads me to this conclusion, because I was not expecting gpt-4o-mini to outperfom gpt-4o!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "017397fb1d264a55a5aff42cd2600226": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "029cbff8d3ab423aaf51d2a62f285bf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "569d02b2e1b14d639d0341fb1cb96e8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "795101f3f9d242fd9633bbbc8f36f08e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cbc0bc6cd564bc1b8153b6ec1742cd4": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_017397fb1d264a55a5aff42cd2600226",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠼</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
         "text/plain": "\u001b[38;2;106;0;255m⠼\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "8afc66dbf89641aba9f4041b8b334eee": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_90b6385e7acb42878af9995a46789a2f",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
         "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "90b6385e7acb42878af9995a46789a2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99c810c345144611a574b0354978952d": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_9beb2254b21c408c905f63b635acef79",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠇</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
         "text/plain": "\u001b[38;2;106;0;255m⠇\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "9ae13990f4b94589bedfecf87ccfae38": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_569d02b2e1b14d639d0341fb1cb96e8f",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠏</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
         "text/plain": "\u001b[38;2;106;0;255m⠏\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "9beb2254b21c408c905f63b635acef79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7e41a0b97244238ad271140f9b7755e": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_029cbff8d3ab423aaf51d2a62f285bf1",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠋</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
         "text/plain": "\u001b[38;2;106;0;255m⠋\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "e1228e602142478db648aad0ff915991": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_795101f3f9d242fd9633bbbc8f36f08e",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">⠙</span> ✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>\n</pre>\n",
         "text/plain": "\u001b[38;2;106;0;255m⠙\u001b[0m ✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
